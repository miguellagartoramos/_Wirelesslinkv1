@Code
    Layout = "~/Views/Shared/_Layout.vbhtml"
    ViewBag.Title = "Rack Fans & Cooling"
End Code

<section class="product-item cable-organizer">
    <div class="container">
        <ol class="breadcrumb">
            <li><a href="~/Home">Home</a></li>
            <li><a href="~/RackEnclosures">Racks and Enclosures</a></li>
            <li><a class="active" href="~/RackEnclosures/rack_fans_and_cooling">Rack Fans & Cooling</a></li>
        </ol>
        <div class="row">
            <div class="col-md-12">
                <div class="product-specification">
                    <h3>Rack Fans & Cooling</h3>
                    <ul>
                        <li><img src="~/img/racks_enclosures/fan_med.jpg" /></li>
                        <li><img src="~/img/racks_enclosures/comp-cool-120.jpg" /></li>
                        <li><img src="~/img/racks_enclosures/fans-quest_med.jpg" /></li>
                        <li><img src="~/img/racks_enclosures/middle-atlantic-quiet-blower-panel_120.jpg" /></li>
                        <li><img src="~/img/racks_enclosures/img-120.jpg" /></li>
                    </ul>
                    <p>Servers generate heat. Lots of it. And heat isn't good for electronics. Many data centers, server rooms and IT closets utilize a finite amount of space, and expanding with additional servers means more heat-generating electronics occupying the same fixed area. Thus, cooling is an extremely important challenge in any networking situation. Thankfully, there are many options available, including ventilation fans, rear doors to maximize air flow, portable air conditioning units, skirts and barriers to help direct and contain hot and cold air, and more. We also offer controls that help monitor temperatures and can automatically turn on fans and cooling equipment when needed.</p>
                </div>
            </div>
        </div>

        <div class="row">
            <h2>Let's Talk About Data Center Cooling</h2>
            <div class="col-md-6">
                <p>When it comes to data centers and server rooms, temperature management is one of the most important challenges that must be tackled. The simple fact is that heat isn't very good for electronics, and conversely, servers generate a lot of heat. The more servers you pack into a finite space, the more heat you're going to generate. Overheating can lead to damaged components, not to mention downtime, both of which can have dire consequences. There are many ways to manage heat and introduce cooling into a server environment, from air conditioning to layout design. We'll take a look at them, but first let's go into a little history.</p>
            </div>
            <div class="col-md-6">
                <h4><strong>I.</strong> In the Beginning</h4>
                <p>One of the early methods of cooling data centers was the "chaos" air distribution method…catchy name, right? In this method, you had computer room air conditioning (CRAC) units around the perimeter of the data center that would supply lots and lots of cold air into the room, which would cool the equipment and push hot air toward the return ducts to be cooled by the CRACs. This wasn't ideal for a lot of reasons. The cool air didn't always get where it was most needed, and the hot air didn't always make it to the return ducts, instead getting into server air intakes and making equipment dangerously hot. Additionally, in an effort to deliver more cool air, the CRACs would pump out air too quickly for the server intakes to draw it in, so it would end up funneled right back into the return ducts before it even had a chance to cool the equipment. Obviously, this wasn't sustainable, so eventually a layout was developed that helped deal with these issues.</p>

                <h4><strong>II.</strong> You're Hot, Then You're Cold</h4>
                <p>The first steps you can take to manage your airflow have to do with how your area is set up. The development of "Hot aisle/cold aisle" helped solve a lot of the problems with the "chaos" method, and has become the go-to layout for servers. Basically, you set up your equipment so that the intakes (for cold air) are facing toward each other and the exhausts (for hot air) face each other (or the wall AC units). The area with exhausts facing become "hot aisles" while those with intakes facing each other are the "cold aisles". Typically, air from the hot aisle is captured by CRAC units, cooled, and then fed into the cold aisle via perforated raised flooring systems</p>

                <h4><strong>III.</strong> It's Gettin' Hot in Here</h4>
                <p>There's still a problem with hot aisle/cold aisle, however: hot air and cold air can still meander into undesired areas. That's why, after your aisles are set up in this configuration, you'll need to institute a containment method to keep the cold air away from the hot air and vice versa. Think of it like Ghostbusters, only instead of trying to keep a bunch of spooky specters from running amok in New York City, you're trying to keep the hot air from invading your cool spaces and ruining its chill vibe. There are many items available to aid in managing the airflow in your server environment. </p>

                <h4><strong>IV.</strong> Think Beyond the CRAC</h4>
                <p>While CRAC units are still used to cool data centers, there are many supplemental cooling products that complement a CRAC unit as well. Portable air conditioning units, such as the ClimateCab AC units by Black Box, can attach directly to racks or enclosures to cool components, rather than the area around them. There are also a wide variety of fan cooling options available. Fans can help keep individual components cool during operation, and are an option for rack set ups in confined spaces, where larger AC units may not be needed or practical. They typically take up as little as 1U of rack space and can install directly into your rack or enclosure.</p>

                <h4><strong>V.</strong> Data Center Feng Shui</h4>
                <p>A final word on data center design: whenever possible, it's ideal to design the room around the equipment, not vice versa. In practicality, of course, this isn't always possible. However, the traditional method of shoving as much IT and cooling equipment in a room as is physically possible has some obvious flaws that could cause serious issues down the road. We can't all be Google and Facebook, obviously, designing data centers the size of giant warehouses to fit our server demands. However, it's still a good idea not to overload a space with more equipment than it can handle. You'll end up paying in the long run, so be sure that your cooling power is able to meet the demands of your heat generation, in a way that's efficient enough not to break your budget.</p>
                <span class="note">*If you need help selecting a solution for your cable management needs, feel free to call 02.727.2727</span>
            </div>
        </div>
    </div>
</section>